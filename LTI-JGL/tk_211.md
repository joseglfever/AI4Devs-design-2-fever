**[Caso de uso](./uc_2.md) | [Historia de usuario](./us_21.md) | [Product backlog](./product_backlog.md) | [Tickets de historia de usuario](./tk_210.moc.md)**

# TK-211: Implementación de servicio de extracción y procesamiento de CVs

## Tipo
Feature

## Prioridad
Alta

## Story Points
13

## Descripción técnica
Desarrollar un nuevo microservicio responsable de procesar documentos CV en múltiples formatos (PDF, DOCX, TXT) y extraer información estructurada mediante técnicas de IA. El servicio debe ser capaz de identificar y categorizar datos clave del currículum como experiencia laboral, habilidades técnicas y blandas, educación, certificaciones y logros profesionales.

### Alcance funcional
- Procesamiento de documentos en formatos PDF, DOCX y TXT
- Extracción de texto preservando estructura semántica del documento
- Identificación de secciones clave mediante NLP
- Categorización y normalización de información extraída
- Sistema de colas para procesamiento asíncrono
- Manejo de errores y reintentos

### Consideraciones técnicas
- Implementar usando Python 3.11+ y FastAPI
- Utilizar Apache Tika para extracción inicial de texto
- Implementar procesamiento asíncrono usando RabbitMQ
- Almacenar documentos originales en Object Storage
- Utilizar modelos de NLP pre-entrenados para clasificación de secciones
- Implementar circuit breakers para servicios externos
- Logging extensivo de errores y métricas de procesamiento

## Criterios de aceptación técnicos
1. API REST:
   - Endpoint POST `/documents` para recibir CVs
   - Endpoint GET `/documents/{id}/status` para consultar estado
   - Endpoint GET `/documents/{id}/extracted-data` para obtener resultados
   - Validación de tipos MIME soportados
   - Límite configurable de tamaño de archivo

2. Procesamiento de documentos:
   - Extracción correcta de texto y metadata
   - Preservación de estructura jerárquica
   - Detección de idioma
   - Manejo de encodings
   - Timeout configurable por documento

3. Extracción de información:
   - Identificación de datos personales y contacto
   - Extracción de experiencia laboral con fechas
   - Identificación de habilidades técnicas y blandas
   - Detección de educación y certificaciones
   - Precisión >95% en campos básicos
   - Confidence score por campo extraído

4. Sistema de colas:
   - Reintentos automáticos configurables
   - Dead letter queue para fallos permanentes
   - Monitoreo de longitud de cola
   - Escalado horizontal de workers

5. Almacenamiento:
   - Persistencia de documentos originales
   - Versionado de extracciones
   - Limpieza automática de archivos temporales
   - Backup periódico de datos procesados

## Servicios/componentes afectados
- Nuevo microservicio: cv-processor-service
- Message broker (RabbitMQ)
- Object Storage service
- Logging service
- Monitoring service

## Arquitectura
```
[Cliente] -> [API Gateway] -> [cv-processor-service] -> [RabbitMQ]
                                       |
                                       v
                             [Worker Pool] -> [Object Storage]
                                       |
                                       v
                             [Results Database]
```

## Dependencias
- Ninguna (ticket inicial del feature)

## Riesgos
- Alto consumo de recursos en procesamiento de PDFs complejos
- Posibles timeouts en documentos muy grandes
- Dependencia de disponibilidad de servicios externos
- Variabilidad en estructura de CVs puede afectar precisión

## Estimación de esfuerzo
- Análisis y diseño: 2 días
- Implementación core: 5 días
- Integración y pruebas: 3 días
- Buffer para imprevistos: 3 días

## Métricas de monitoreo
- Tiempo promedio de procesamiento
- Tasa de éxito/error
- Precisión de extracción por campo
- Uso de recursos (CPU/memoria)
- Longitud de cola de procesamiento
- Latencia de API endpoints

**[Caso de uso](./uc_2.md) | [Historia de usuario](./us_21.md) | [Product backlog](./product_backlog.md) | [Tickets de historia de usuario](./tk_210.moc.md)**
